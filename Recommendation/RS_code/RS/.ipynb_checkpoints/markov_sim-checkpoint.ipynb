{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#markov similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.\n: java.lang.NullPointerException\n\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:64)\n\tat org.apache.spark.storage.BlockManager.initialize(BlockManager.scala:243)\n\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:509)\n\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:238)\n\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-4478a20a864d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSparkConf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"spark://39.99.129.214:7077\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"SecondApp\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mspark\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaster\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"spark://39.99.129.214:7077\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSparkConf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/context.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n\u001b[0;32m--> 132\u001b[0;31m                           conf, jsc, profiler_cls)\n\u001b[0m\u001b[1;32m    133\u001b[0m         \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m             \u001b[0;31m# If an error occurs, clean up in order to allow future SparkContext creation:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/context.py\u001b[0m in \u001b[0;36m_do_init\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, jsc, profiler_cls)\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m         \u001b[0;31m# Create the Java SparkContext through Py4J\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 194\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjsc\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    195\u001b[0m         \u001b[0;31m# Reset the SparkConf to the one actually used by the SparkContext in JVM.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkConf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_jconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/context.py\u001b[0m in \u001b[0;36m_initialize_context\u001b[0;34m(self, jconf)\u001b[0m\n\u001b[1;32m    300\u001b[0m         \u001b[0mInitialize\u001b[0m \u001b[0mSparkContext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfunction\u001b[0m \u001b[0mto\u001b[0m \u001b[0mallow\u001b[0m \u001b[0msubclass\u001b[0m \u001b[0mspecific\u001b[0m \u001b[0minitialization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m         \"\"\"\n\u001b[0;32m--> 302\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mJavaSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1523\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1524\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1525\u001b[0;31m             answer, self._gateway_client, None, self._fqn)\n\u001b[0m\u001b[1;32m   1526\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1527\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.\n: java.lang.NullPointerException\n\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:64)\n\tat org.apache.spark.storage.BlockManager.initialize(BlockManager.scala:243)\n\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:509)\n\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:238)\n\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf\n",
    "sc = SparkContext(\"spark://39.99.129.214:7077\", \"SecondApp\")\n",
    "spark = SparkSession.builder.master(\"spark://39.99.129.214:7077\").config(conf=SparkConf()).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "import pymongo\n",
    "import pandas as pd\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark import SparkContext\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "\n",
    "'''\n",
    "specify collection name, here:\n",
    "    ratings_temp: original ml-20m ratings\n",
    "    ratings_sort: sort ratings_temp by userId (ascend) and timestamp (ascend)\n",
    "'''\n",
    "def read_mongo(collection):\n",
    "    client = pymongo.MongoClient(\"39.98.136.173\", 9099)\n",
    "    client.movie.authenticate('user','cloud',mechanism='SCRAM-SHA-1')\n",
    "    database = client['movie']\n",
    "    ratings = database['ratings_sort']\n",
    "    cursor = ratings.find()\n",
    "    cnt = 0\n",
    "    #df = pd.DataFrame(columns=('userId','movieId','rating','timestamp'))\n",
    "    df = [] \n",
    "    for i in tqdm(range(2000000)):\n",
    "        #userId = copy.deepcopy(cursor[i]['userId'])\n",
    "        movieId = cursor[i]['movieId']\n",
    "        #rating = copy.deepcopy(cursor[i]['rating'])\n",
    "        #timestamp = copy.deepcopy(cursor[i]['timestamp'])\n",
    "    #         print(userId,movieId,rating,timestamp)\n",
    "    #         new = pd.DataFrame({'userId':[userId],'movieId':[movieId],'rating':[rating],'timestamp':[timestamp]})\n",
    "    #         print(new)\n",
    "        #df.append(pd.DataFrame(record,index=[0]),ignore_index=True)\n",
    "        df.append([movieId])\n",
    "        cnt = cnt + 1\n",
    "        if cnt == 2000000:\n",
    "            break\n",
    "    \n",
    "#     df = pd.DataFrame(tqdm(list(cursor)))\n",
    "#     df = df[['userId', 'movieId', 'rating', 'timestamp']]\n",
    "#     spark = SparkSession.builder.appName('readMongo').getOrCreate()\n",
    "#     sqlContest = SQLContext(spark)\n",
    "#     print(\"creating spark dataframe......\")\n",
    "#     spark_df = sqlContest.createDataFrame(df)\n",
    "#     return spark_df\n",
    "df = read_mongo('ratings_sort')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df = pd.read_csv('/usr/dataset/ml-20m/ratings.csv')\n",
    "df['userId'] = df['movieId'].shift(1)\n",
    "df = df.fillna(1)\n",
    "df['userId'] = np.array(df['userId'].values,dtype=np.int)\n",
    "df['rating'] = np.random.randint(0,2,len(df))\n",
    "df[['userId','movieId','rating']].to_csv('/usr/dataset/ml-20m/markov.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('RS').getOrCreate()\n",
    "# load dataset\n",
    "df = spark.read.csv('/usr/dataset/ml-20m/markov.csv', inferSchema=True, header=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-------+------+\n",
      "|_c0|userId|movieId|rating|\n",
      "+---+------+-------+------+\n",
      "|  0|     1|      2|     0|\n",
      "|  1|     2|     29|     1|\n",
      "|  2|    29|     32|     1|\n",
      "|  3|    32|     47|     0|\n",
      "|  4|    47|     50|     0|\n",
      "|  5|    50|    112|     0|\n",
      "|  6|   112|    151|     1|\n",
      "|  7|   151|    223|     1|\n",
      "|  8|   223|    253|     1|\n",
      "|  9|   253|    260|     1|\n",
      "+---+------+-------+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "userCount_df = df.groupBy('userId').count().orderBy('count', ascending=False)\n",
    "# Spark's DataFrame --> Pandas's DataFrame\n",
    "user_df = userCount_df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-------+------+-----------+\n",
      "|_c0|userId|movieId|rating|movieId_num|\n",
      "+---+------+-------+------+-----------+\n",
      "|  0|     1|      2|     0|      125.0|\n",
      "|  1|     2|     29|     1|      564.0|\n",
      "|  2|    29|     32|     1|       19.0|\n",
      "|  3|    32|     47|     0|       23.0|\n",
      "|  4|    47|     50|     0|       14.0|\n",
      "|  5|    50|    112|     0|      370.0|\n",
      "|  6|   112|    151|     1|      335.0|\n",
      "|  7|   151|    223|     1|      106.0|\n",
      "|  8|   223|    253|     1|       77.0|\n",
      "|  9|   253|    260|     1|        5.0|\n",
      "+---+------+-------+------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.ml.feature import StringIndexer, IndexToString\n",
    "stringIndexer = StringIndexer(inputCol='movieId', outputCol='movieId_num')\n",
    "model = stringIndexer.fit(df)\n",
    "new_df  = model.transform(df)\n",
    "new_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_df, (13996257, 5)\n",
      "test_df, (6004006, 5)\n"
     ]
    }
   ],
   "source": [
    "train_df, test_df = new_df.randomSplit([0.7, 0.3])\n",
    "\n",
    "print('train_df, (%d, %d)'%(train_df.count(), len(train_df.columns)))\n",
    "print('test_df, (%d, %d)'%(test_df.count(), len(test_df.columns)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.recommendation import ALS\n",
    "#train for model\n",
    "rec = ALS(maxIter=10, regParam=0.01, userCol='userId', itemCol='movieId_num', ratingCol='rating', nonnegative=True,\n",
    "                 coldStartStrategy='drop')\n",
    "rs_model = rec.fit(train_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+-----------+----------+\n",
      "|userId|movieId|movieId_num|prediction|\n",
      "+------+-------+-----------+----------+\n",
      "|  1591|     17|      148.0|  0.466884|\n",
      "|  1127|     17|      148.0|0.45668304|\n",
      "|  2563|     17|      148.0|0.46466967|\n",
      "|  4161|     17|      148.0|0.47137365|\n",
      "|  4929|     17|      148.0|0.51497185|\n",
      "| 48780|     17|      148.0|0.45662728|\n",
      "| 56941|     17|      148.0| 0.5391659|\n",
      "|  2996|     17|      148.0|0.46675533|\n",
      "|  6482|     17|      148.0|0.49000967|\n",
      "| 76293|     17|      148.0| 0.5333507|\n",
      "|  8665|     17|      148.0|0.46893552|\n",
      "|101142|     17|      148.0|0.44186097|\n",
      "|128830|     17|      148.0|0.26811567|\n",
      "|  5995|     17|      148.0| 0.4856425|\n",
      "|  6722|     17|      148.0| 0.5723945|\n",
      "|  6832|     17|      148.0|0.46817586|\n",
      "| 27822|     17|      148.0|0.46095657|\n",
      "|  1466|     17|      148.0| 0.4438986|\n",
      "|  4396|     17|      148.0|0.44765642|\n",
      "|120132|     17|      148.0|0.19979942|\n",
      "+------+-------+-----------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#test for model\n",
    "test = test_df.select(['userId','movieId','movieId_num'])\n",
    "test_pred = rs_model.transform(test)\n",
    "test_pred.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test rmse is 0.502530\n"
     ]
    }
   ],
   "source": [
    "#test rmse for model\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "test_pred = rs_model.transform(test_df)\n",
    "\n",
    "evaluate_result = RegressionEvaluator(metricName='rmse', predictionCol='prediction', labelCol='rating')\n",
    "rmse = evaluate_result.evaluate(test_pred)\n",
    "print('test rmse is %f'%rmse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+\n",
      "|movieId|userId|\n",
      "+-------+------+\n",
      "|148    |66    |\n",
      "|463    |66    |\n",
      "|471    |66    |\n",
      "|496    |66    |\n",
      "|833    |66    |\n",
      "|1088   |66    |\n",
      "|1238   |66    |\n",
      "|1342   |66    |\n",
      "|1580   |66    |\n",
      "|1591   |66    |\n",
      "+-------+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#e.g. recommend movie for user 66\n",
    "# total movies\n",
    "nunique_movies = new_df.select('movieId').distinct()\n",
    "a = nunique_movies.alias('a')\n",
    "user_id = 66\n",
    "# user_id = 66，看过的电影\n",
    "watched_movies = new_df.filter(new_df['userId'] == user_id).select('movieId').distinct()\n",
    "b = watched_movies.alias('b')\n",
    "# a join b\n",
    "total_movies = a.join(b, a.movieId == b.movieId, how='left')\n",
    "user_66_not_watched_movies = total_movies.where(col('b.movieId').isNull()).select(a.movieId).distinct()\n",
    "user_66_not_watched_movies = user_66_not_watched_movies.withColumn('userId', lit(int(user_id)))\n",
    "user_66_not_watched_movies.show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#recommendation for all users\n",
    "rec = rs_model.recommendForAllUsers(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rec= rec.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rec = df_rec.sort_values(by='userId')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>recommendations</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5355</th>\n",
       "      <td>1</td>\n",
       "      <td>[(14672, 1.1313905715942383), (17758, 1.082917...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22045</th>\n",
       "      <td>2</td>\n",
       "      <td>[(18198, 1.1249234676361084), (18792, 1.076898...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6363</th>\n",
       "      <td>3</td>\n",
       "      <td>[(18804, 1.0842939615249634), (18198, 1.082339...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12871</th>\n",
       "      <td>4</td>\n",
       "      <td>[(18198, 1.1037191152572632), (18804, 1.073117...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8245</th>\n",
       "      <td>5</td>\n",
       "      <td>[(21832, 1.0734281539916992), (18804, 1.064790...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11862</th>\n",
       "      <td>131248</td>\n",
       "      <td>[(13951, 1.4968010187149048), (15278, 1.471555...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3219</th>\n",
       "      <td>131256</td>\n",
       "      <td>[(0, 0.0), (10, 0.0), (20, 0.0), (30, 0.0), (4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24171</th>\n",
       "      <td>131258</td>\n",
       "      <td>[(0, 0.0), (10, 0.0), (20, 0.0), (30, 0.0), (4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15295</th>\n",
       "      <td>131260</td>\n",
       "      <td>[(18792, 2.265856981277466), (14672, 2.2063508...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19746</th>\n",
       "      <td>131262</td>\n",
       "      <td>[(0, 0.0), (10, 0.0), (20, 0.0), (30, 0.0), (4...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25297 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       userId                                    recommendations\n",
       "5355        1  [(14672, 1.1313905715942383), (17758, 1.082917...\n",
       "22045       2  [(18198, 1.1249234676361084), (18792, 1.076898...\n",
       "6363        3  [(18804, 1.0842939615249634), (18198, 1.082339...\n",
       "12871       4  [(18198, 1.1037191152572632), (18804, 1.073117...\n",
       "8245        5  [(21832, 1.0734281539916992), (18804, 1.064790...\n",
       "...       ...                                                ...\n",
       "11862  131248  [(13951, 1.4968010187149048), (15278, 1.471555...\n",
       "3219   131256  [(0, 0.0), (10, 0.0), (20, 0.0), (30, 0.0), (4...\n",
       "24171  131258  [(0, 0.0), (10, 0.0), (20, 0.0), (30, 0.0), (4...\n",
       "15295  131260  [(18792, 2.265856981277466), (14672, 2.2063508...\n",
       "19746  131262  [(0, 0.0), (10, 0.0), (20, 0.0), (30, 0.0), (4...\n",
       "\n",
       "[25297 rows x 2 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "i = 0\n",
    "res = []\n",
    "for lines in df_rec.values:\n",
    "    userID = lines[0]\n",
    "    items = [line[0] for line in lines[1]]\n",
    "    while userID != i:\n",
    "        res.append(np.random.randint(0,131262,10).tolist())\n",
    "        i = i + 1\n",
    "    \n",
    "    res.append(items)\n",
    "    i = i +1 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "131263"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "fw = open('markov_similarity.pkl','wb')  \n",
    "pickle.dump(res, fw, -1)  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
